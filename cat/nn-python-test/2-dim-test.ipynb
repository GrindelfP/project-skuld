{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68836988-588a-4929-bfea-e24c717c1099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "080e0c47-d99c-4619-b8e1-ca64997e0db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(func, n_samples=100, n_dim=1):\n",
    "    X = torch.linspace(0, 1, n_samples * n_dim).unsqueeze(1)\n",
    "    shuffled_indices = torch.randperm(X.size(0))\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    \n",
    "    split_X = torch.chunk(X, n_dim)        \n",
    "    X = torch.stack(split_X, dim=1)\n",
    "\n",
    "    y = func(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c043ec8b-ea03-4278-ba7a-0bbe30a5d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(X):\n",
    "    # Extract x1 and x2 from the columns of the 2D tensor X\n",
    "    x1 = X[:, 0]  # First column\n",
    "    x2 = X[:, 1]  # Second column\n",
    "    \n",
    "    return torch.cos(x1) + torch.sin(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e12848d-ae50-4f93-9fd8-ba0122059ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_data(f1, 10000, n_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31bc935-0da1-47c8-88c3-e7b632df3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layer = nn.Sigmoid()\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17f2896-175c-4069-9f26-d8102be34593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "def train_model(model, criterion, optimizer, x_train, y_train, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Прямой проход\n",
    "        predictions = model(x_train)\n",
    "        loss = criterion(predictions, y_train)\n",
    "\n",
    "        # Обратное распространение и обновление весов\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Печать потерь\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8afab6a-15b0-4802-b363-8896b7959e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гиперпараметры\n",
    "input_size = 2  # Одна переменная x\n",
    "hidden_size = 10  # Количество нейронов в скрытом слое\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10000\n",
    "\n",
    "# Создаем модель\n",
    "model = MLP(input_size, hidden_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2efdd7a4-f6be-4288-824a-85df561ee101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация данных\n",
    "x_train, y_train = generate_data(exp_func, 10000)  # Используем cos(x) или exp_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f40a0855-2e3c-43e0-97d0-c025adef8ec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 0.128916\n",
      "Epoch [200/10000], Loss: 0.059543\n",
      "Epoch [300/10000], Loss: 0.015104\n",
      "Epoch [400/10000], Loss: 0.004839\n",
      "Epoch [500/10000], Loss: 0.003649\n",
      "Epoch [600/10000], Loss: 0.003084\n",
      "Epoch [700/10000], Loss: 0.002617\n",
      "Epoch [800/10000], Loss: 0.002222\n",
      "Epoch [900/10000], Loss: 0.001885\n",
      "Epoch [1000/10000], Loss: 0.001593\n",
      "Epoch [1100/10000], Loss: 0.001338\n",
      "Epoch [1200/10000], Loss: 0.001114\n",
      "Epoch [1300/10000], Loss: 0.000917\n",
      "Epoch [1400/10000], Loss: 0.000747\n",
      "Epoch [1500/10000], Loss: 0.000600\n",
      "Epoch [1600/10000], Loss: 0.000475\n",
      "Epoch [1700/10000], Loss: 0.000372\n",
      "Epoch [1800/10000], Loss: 0.000288\n",
      "Epoch [1900/10000], Loss: 0.000221\n",
      "Epoch [2000/10000], Loss: 0.000169\n",
      "Epoch [2100/10000], Loss: 0.000130\n",
      "Epoch [2200/10000], Loss: 0.000101\n",
      "Epoch [2300/10000], Loss: 0.000080\n",
      "Epoch [2400/10000], Loss: 0.000066\n",
      "Epoch [2500/10000], Loss: 0.000056\n",
      "Epoch [2600/10000], Loss: 0.000048\n",
      "Epoch [2700/10000], Loss: 0.000043\n",
      "Epoch [2800/10000], Loss: 0.000040\n",
      "Epoch [2900/10000], Loss: 0.000037\n",
      "Epoch [3000/10000], Loss: 0.000035\n",
      "Epoch [3100/10000], Loss: 0.000032\n",
      "Epoch [3200/10000], Loss: 0.000031\n",
      "Epoch [3300/10000], Loss: 0.000029\n",
      "Epoch [3400/10000], Loss: 0.000027\n",
      "Epoch [3500/10000], Loss: 0.000026\n",
      "Epoch [3600/10000], Loss: 0.000024\n",
      "Epoch [3700/10000], Loss: 0.000023\n",
      "Epoch [3800/10000], Loss: 0.000022\n",
      "Epoch [3900/10000], Loss: 0.000021\n",
      "Epoch [4000/10000], Loss: 0.000019\n",
      "Epoch [4100/10000], Loss: 0.000018\n",
      "Epoch [4200/10000], Loss: 0.000017\n",
      "Epoch [4300/10000], Loss: 0.000016\n",
      "Epoch [4400/10000], Loss: 0.000015\n",
      "Epoch [4500/10000], Loss: 0.000014\n",
      "Epoch [4600/10000], Loss: 0.000013\n",
      "Epoch [4700/10000], Loss: 0.000012\n",
      "Epoch [4800/10000], Loss: 0.000012\n",
      "Epoch [4900/10000], Loss: 0.000011\n",
      "Epoch [5000/10000], Loss: 0.000010\n",
      "Epoch [5100/10000], Loss: 0.000009\n",
      "Epoch [5200/10000], Loss: 0.000009\n",
      "Epoch [5300/10000], Loss: 0.000008\n",
      "Epoch [5400/10000], Loss: 0.000008\n",
      "Epoch [5500/10000], Loss: 0.000007\n",
      "Epoch [5600/10000], Loss: 0.000006\n",
      "Epoch [5700/10000], Loss: 0.000006\n",
      "Epoch [5800/10000], Loss: 0.000006\n",
      "Epoch [5900/10000], Loss: 0.000005\n",
      "Epoch [6000/10000], Loss: 0.000005\n",
      "Epoch [6100/10000], Loss: 0.000004\n",
      "Epoch [6200/10000], Loss: 0.000004\n",
      "Epoch [6300/10000], Loss: 0.000004\n",
      "Epoch [6400/10000], Loss: 0.000003\n",
      "Epoch [6500/10000], Loss: 0.000003\n",
      "Epoch [6600/10000], Loss: 0.000003\n",
      "Epoch [6700/10000], Loss: 0.000003\n",
      "Epoch [6800/10000], Loss: 0.000002\n",
      "Epoch [6900/10000], Loss: 0.000002\n",
      "Epoch [7000/10000], Loss: 0.000002\n",
      "Epoch [7100/10000], Loss: 0.000002\n",
      "Epoch [7200/10000], Loss: 0.000002\n",
      "Epoch [7300/10000], Loss: 0.000002\n",
      "Epoch [7400/10000], Loss: 0.000002\n",
      "Epoch [7500/10000], Loss: 0.000002\n",
      "Epoch [7600/10000], Loss: 0.000001\n",
      "Epoch [7700/10000], Loss: 0.000001\n",
      "Epoch [7800/10000], Loss: 0.000001\n",
      "Epoch [7900/10000], Loss: 0.000001\n",
      "Epoch [8000/10000], Loss: 0.000001\n",
      "Epoch [8100/10000], Loss: 0.000001\n",
      "Epoch [8200/10000], Loss: 0.000002\n",
      "Epoch [8300/10000], Loss: 0.000001\n",
      "Epoch [8400/10000], Loss: 0.000001\n",
      "Epoch [8500/10000], Loss: 0.000001\n",
      "Epoch [8600/10000], Loss: 0.000001\n",
      "Epoch [8700/10000], Loss: 0.000001\n",
      "Epoch [8800/10000], Loss: 0.000001\n",
      "Epoch [8900/10000], Loss: 0.000001\n",
      "Epoch [9000/10000], Loss: 0.000001\n",
      "Epoch [9100/10000], Loss: 0.000001\n",
      "Epoch [9200/10000], Loss: 0.000001\n",
      "Epoch [9300/10000], Loss: 0.000001\n",
      "Epoch [9400/10000], Loss: 0.000008\n",
      "Epoch [9500/10000], Loss: 0.000001\n",
      "Epoch [9600/10000], Loss: 0.000001\n",
      "Epoch [9700/10000], Loss: 0.000002\n",
      "Epoch [9800/10000], Loss: 0.000001\n",
      "Epoch [9900/10000], Loss: 0.000001\n",
      "Epoch [10000/10000], Loss: 0.000001\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "train_model(model, criterion, optimizer, x_train, y_train, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce2befc1-cdce-4e6a-9812-18f0916b46af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Приближенное значение интеграла: 1.718310\n"
     ]
    }
   ],
   "source": [
    "# Вычисление интеграла\n",
    "integral = integrate_model(model)\n",
    "print(f'Приближенное значение интеграла: {integral:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "680a638e-3030-4dfc-9ff1-77d526e7072e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точное значение интеграла: 1.718282\n"
     ]
    }
   ],
   "source": [
    "# Для сравнения: точное значение интеграла\n",
    "true_integral = np.exp(1) - np.exp(0)  # Для cos(x)\n",
    "print(f'Точное значение интеграла: {true_integral.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d5e0f84-55d9-4ea1-b4cf-4a9509556ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аналитическое значение интеграла нейросети: 1.718296\n"
     ]
    }
   ],
   "source": [
    "# Вычисление интеграла аналитически\n",
    "integral_analytical = integrate_model_analytically(model)\n",
    "print(f'Аналитическое значение интеграла нейросети: {integral_analytical:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ee478-18c7-40ae-8550-e6e5e3fc5d5d",
   "metadata": {},
   "source": [
    "## 1D cos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "218d1c52-667f-4888-9862-603d2fcacdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель\n",
    "model = MLP(input_size, hidden_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "797d37ae-5f8f-4eb0-a208-857d446eab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация данных\n",
    "x_train, y_train = generate_data(cos_func, 10000)  # Используем cos(x) или exp_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "df6edcaf-93b5-4e8b-95dd-3790e131a96a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 0.007816\n",
      "Epoch [200/10000], Loss: 0.002053\n",
      "Epoch [300/10000], Loss: 0.001363\n",
      "Epoch [400/10000], Loss: 0.001294\n",
      "Epoch [500/10000], Loss: 0.001235\n",
      "Epoch [600/10000], Loss: 0.001180\n",
      "Epoch [700/10000], Loss: 0.001129\n",
      "Epoch [800/10000], Loss: 0.001082\n",
      "Epoch [900/10000], Loss: 0.001039\n",
      "Epoch [1000/10000], Loss: 0.000999\n",
      "Epoch [1100/10000], Loss: 0.000959\n",
      "Epoch [1200/10000], Loss: 0.000920\n",
      "Epoch [1300/10000], Loss: 0.000881\n",
      "Epoch [1400/10000], Loss: 0.000840\n",
      "Epoch [1500/10000], Loss: 0.000796\n",
      "Epoch [1600/10000], Loss: 0.000749\n",
      "Epoch [1700/10000], Loss: 0.000697\n",
      "Epoch [1800/10000], Loss: 0.000639\n",
      "Epoch [1900/10000], Loss: 0.000576\n",
      "Epoch [2000/10000], Loss: 0.000507\n",
      "Epoch [2100/10000], Loss: 0.000434\n",
      "Epoch [2200/10000], Loss: 0.000358\n",
      "Epoch [2300/10000], Loss: 0.000282\n",
      "Epoch [2400/10000], Loss: 0.000211\n",
      "Epoch [2500/10000], Loss: 0.000149\n",
      "Epoch [2600/10000], Loss: 0.000098\n",
      "Epoch [2700/10000], Loss: 0.000061\n",
      "Epoch [2800/10000], Loss: 0.000036\n",
      "Epoch [2900/10000], Loss: 0.000021\n",
      "Epoch [3000/10000], Loss: 0.000013\n",
      "Epoch [3100/10000], Loss: 0.000008\n",
      "Epoch [3200/10000], Loss: 0.000006\n",
      "Epoch [3300/10000], Loss: 0.000005\n",
      "Epoch [3400/10000], Loss: 0.000005\n",
      "Epoch [3500/10000], Loss: 0.000005\n",
      "Epoch [3600/10000], Loss: 0.000005\n",
      "Epoch [3700/10000], Loss: 0.000005\n",
      "Epoch [3800/10000], Loss: 0.000005\n",
      "Epoch [3900/10000], Loss: 0.000004\n",
      "Epoch [4000/10000], Loss: 0.000004\n",
      "Epoch [4100/10000], Loss: 0.000004\n",
      "Epoch [4200/10000], Loss: 0.000004\n",
      "Epoch [4300/10000], Loss: 0.000004\n",
      "Epoch [4400/10000], Loss: 0.000004\n",
      "Epoch [4500/10000], Loss: 0.000004\n",
      "Epoch [4600/10000], Loss: 0.000004\n",
      "Epoch [4700/10000], Loss: 0.000004\n",
      "Epoch [4800/10000], Loss: 0.000004\n",
      "Epoch [4900/10000], Loss: 0.000004\n",
      "Epoch [5000/10000], Loss: 0.000004\n",
      "Epoch [5100/10000], Loss: 0.000004\n",
      "Epoch [5200/10000], Loss: 0.000004\n",
      "Epoch [5300/10000], Loss: 0.000004\n",
      "Epoch [5400/10000], Loss: 0.000004\n",
      "Epoch [5500/10000], Loss: 0.000004\n",
      "Epoch [5600/10000], Loss: 0.000004\n",
      "Epoch [5700/10000], Loss: 0.000004\n",
      "Epoch [5800/10000], Loss: 0.000004\n",
      "Epoch [5900/10000], Loss: 0.000004\n",
      "Epoch [6000/10000], Loss: 0.000004\n",
      "Epoch [6100/10000], Loss: 0.000004\n",
      "Epoch [6200/10000], Loss: 0.000004\n",
      "Epoch [6300/10000], Loss: 0.000003\n",
      "Epoch [6400/10000], Loss: 0.000003\n",
      "Epoch [6500/10000], Loss: 0.000054\n",
      "Epoch [6600/10000], Loss: 0.000003\n",
      "Epoch [6700/10000], Loss: 0.000003\n",
      "Epoch [6800/10000], Loss: 0.000003\n",
      "Epoch [6900/10000], Loss: 0.000003\n",
      "Epoch [7000/10000], Loss: 0.000003\n",
      "Epoch [7100/10000], Loss: 0.000003\n",
      "Epoch [7200/10000], Loss: 0.000003\n",
      "Epoch [7300/10000], Loss: 0.000003\n",
      "Epoch [7400/10000], Loss: 0.000003\n",
      "Epoch [7500/10000], Loss: 0.000003\n",
      "Epoch [7600/10000], Loss: 0.000003\n",
      "Epoch [7700/10000], Loss: 0.000003\n",
      "Epoch [7800/10000], Loss: 0.000003\n",
      "Epoch [7900/10000], Loss: 0.000004\n",
      "Epoch [8000/10000], Loss: 0.000003\n",
      "Epoch [8100/10000], Loss: 0.000003\n",
      "Epoch [8200/10000], Loss: 0.000003\n",
      "Epoch [8300/10000], Loss: 0.000003\n",
      "Epoch [8400/10000], Loss: 0.000003\n",
      "Epoch [8500/10000], Loss: 0.000005\n",
      "Epoch [8600/10000], Loss: 0.000003\n",
      "Epoch [8700/10000], Loss: 0.000003\n",
      "Epoch [8800/10000], Loss: 0.000006\n",
      "Epoch [8900/10000], Loss: 0.000003\n",
      "Epoch [9000/10000], Loss: 0.000003\n",
      "Epoch [9100/10000], Loss: 0.000003\n",
      "Epoch [9200/10000], Loss: 0.000003\n",
      "Epoch [9300/10000], Loss: 0.000010\n",
      "Epoch [9400/10000], Loss: 0.000003\n",
      "Epoch [9500/10000], Loss: 0.000003\n",
      "Epoch [9600/10000], Loss: 0.000011\n",
      "Epoch [9700/10000], Loss: 0.000003\n",
      "Epoch [9800/10000], Loss: 0.000003\n",
      "Epoch [9900/10000], Loss: 0.000012\n",
      "Epoch [10000/10000], Loss: 0.000003\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "train_model(model, criterion, optimizer, x_train, y_train, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "07ca3cd8-84b9-4657-9690-7bb88534b14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Приближенное значение интеграла: 0.841498\n"
     ]
    }
   ],
   "source": [
    "# Вычисление интеграла\n",
    "integral = integrate_model(model)\n",
    "print(f'Приближенное значение интеграла: {integral:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d3e1fe84-ff9e-46d3-afe9-f796e8556fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точное значение интеграла: 0.841471\n"
     ]
    }
   ],
   "source": [
    "# Для сравнения: точное значение интеграла\n",
    "true_integral = np.sin(1) - np.sin(0)  # Для cos(x)\n",
    "print(f'Точное значение интеграла: {true_integral.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c6fbd096-9403-416d-b375-d9e7c5b987e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аналитическое значение интеграла нейросети: 0.841504\n"
     ]
    }
   ],
   "source": [
    "# Вычисление интеграла аналитически\n",
    "integral_analytical = integrate_model_analytically(model)\n",
    "print(f'Аналитическое значение интеграла нейросети: {integral_analytical:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb272c84-ff7e-4701-8f68-5a88a4b7f522",
   "metadata": {},
   "source": [
    "## 1D$\\frac{t^m}{(1+t)^n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d3aaaf5c-05d3-4663-85ba-f9f4f793425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель\n",
    "model = MLP(input_size, hidden_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "19bc132b-8622-4510-a6da-1fbb2e8ede54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация данных\n",
    "x_train, y_train = generate_data_t_func(10000)  # Используем cos(x) или exp_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b30696fc-2566-4890-8487-903a7c8a6990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 0.004282\n",
      "Epoch [200/10000], Loss: 0.001439\n",
      "Epoch [300/10000], Loss: 0.001329\n",
      "Epoch [400/10000], Loss: 0.001193\n",
      "Epoch [500/10000], Loss: 0.001038\n",
      "Epoch [600/10000], Loss: 0.000875\n",
      "Epoch [700/10000], Loss: 0.000716\n",
      "Epoch [800/10000], Loss: 0.000571\n",
      "Epoch [900/10000], Loss: 0.000447\n",
      "Epoch [1000/10000], Loss: 0.000346\n",
      "Epoch [1100/10000], Loss: 0.000269\n",
      "Epoch [1200/10000], Loss: 0.000213\n",
      "Epoch [1300/10000], Loss: 0.000175\n",
      "Epoch [1400/10000], Loss: 0.000150\n",
      "Epoch [1500/10000], Loss: 0.000134\n",
      "Epoch [1600/10000], Loss: 0.000124\n",
      "Epoch [1700/10000], Loss: 0.000117\n",
      "Epoch [1800/10000], Loss: 0.000112\n",
      "Epoch [1900/10000], Loss: 0.000107\n",
      "Epoch [2000/10000], Loss: 0.000103\n",
      "Epoch [2100/10000], Loss: 0.000099\n",
      "Epoch [2200/10000], Loss: 0.000095\n",
      "Epoch [2300/10000], Loss: 0.000091\n",
      "Epoch [2400/10000], Loss: 0.000088\n",
      "Epoch [2500/10000], Loss: 0.000084\n",
      "Epoch [2600/10000], Loss: 0.000080\n",
      "Epoch [2700/10000], Loss: 0.000076\n",
      "Epoch [2800/10000], Loss: 0.000072\n",
      "Epoch [2900/10000], Loss: 0.000068\n",
      "Epoch [3000/10000], Loss: 0.000065\n",
      "Epoch [3100/10000], Loss: 0.000061\n",
      "Epoch [3200/10000], Loss: 0.000057\n",
      "Epoch [3300/10000], Loss: 0.000053\n",
      "Epoch [3400/10000], Loss: 0.000049\n",
      "Epoch [3500/10000], Loss: 0.000046\n",
      "Epoch [3600/10000], Loss: 0.000042\n",
      "Epoch [3700/10000], Loss: 0.000039\n",
      "Epoch [3800/10000], Loss: 0.000035\n",
      "Epoch [3900/10000], Loss: 0.000032\n",
      "Epoch [4000/10000], Loss: 0.000029\n",
      "Epoch [4100/10000], Loss: 0.000026\n",
      "Epoch [4200/10000], Loss: 0.000023\n",
      "Epoch [4300/10000], Loss: 0.000021\n",
      "Epoch [4400/10000], Loss: 0.000019\n",
      "Epoch [4500/10000], Loss: 0.000017\n",
      "Epoch [4600/10000], Loss: 0.000016\n",
      "Epoch [4700/10000], Loss: 0.000014\n",
      "Epoch [4800/10000], Loss: 0.000014\n",
      "Epoch [4900/10000], Loss: 0.000012\n",
      "Epoch [5000/10000], Loss: 0.000030\n",
      "Epoch [5100/10000], Loss: 0.000011\n",
      "Epoch [5200/10000], Loss: 0.000010\n",
      "Epoch [5300/10000], Loss: 0.000010\n",
      "Epoch [5400/10000], Loss: 0.000009\n",
      "Epoch [5500/10000], Loss: 0.000010\n",
      "Epoch [5600/10000], Loss: 0.000009\n",
      "Epoch [5700/10000], Loss: 0.000009\n",
      "Epoch [5800/10000], Loss: 0.000011\n",
      "Epoch [5900/10000], Loss: 0.000008\n",
      "Epoch [6000/10000], Loss: 0.000008\n",
      "Epoch [6100/10000], Loss: 0.000008\n",
      "Epoch [6200/10000], Loss: 0.000007\n",
      "Epoch [6300/10000], Loss: 0.000098\n",
      "Epoch [6400/10000], Loss: 0.000007\n",
      "Epoch [6500/10000], Loss: 0.000007\n",
      "Epoch [6600/10000], Loss: 0.000154\n",
      "Epoch [6700/10000], Loss: 0.000007\n",
      "Epoch [6800/10000], Loss: 0.000006\n",
      "Epoch [6900/10000], Loss: 0.000010\n",
      "Epoch [7000/10000], Loss: 0.000006\n",
      "Epoch [7100/10000], Loss: 0.000006\n",
      "Epoch [7200/10000], Loss: 0.000009\n",
      "Epoch [7300/10000], Loss: 0.000006\n",
      "Epoch [7400/10000], Loss: 0.000005\n",
      "Epoch [7500/10000], Loss: 0.000025\n",
      "Epoch [7600/10000], Loss: 0.000005\n",
      "Epoch [7700/10000], Loss: 0.000005\n",
      "Epoch [7800/10000], Loss: 0.000005\n",
      "Epoch [7900/10000], Loss: 0.000005\n",
      "Epoch [8000/10000], Loss: 0.000005\n",
      "Epoch [8100/10000], Loss: 0.000004\n",
      "Epoch [8200/10000], Loss: 0.000004\n",
      "Epoch [8300/10000], Loss: 0.000004\n",
      "Epoch [8400/10000], Loss: 0.000116\n",
      "Epoch [8500/10000], Loss: 0.000004\n",
      "Epoch [8600/10000], Loss: 0.000004\n",
      "Epoch [8700/10000], Loss: 0.000007\n",
      "Epoch [8800/10000], Loss: 0.000004\n",
      "Epoch [8900/10000], Loss: 0.000003\n",
      "Epoch [9000/10000], Loss: 0.000010\n",
      "Epoch [9100/10000], Loss: 0.000003\n",
      "Epoch [9200/10000], Loss: 0.000003\n",
      "Epoch [9300/10000], Loss: 0.000010\n",
      "Epoch [9400/10000], Loss: 0.000003\n",
      "Epoch [9500/10000], Loss: 0.000003\n",
      "Epoch [9600/10000], Loss: 0.000003\n",
      "Epoch [9700/10000], Loss: 0.000003\n",
      "Epoch [9800/10000], Loss: 0.000005\n",
      "Epoch [9900/10000], Loss: 0.000003\n",
      "Epoch [10000/10000], Loss: 0.000003\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "train_model(model, criterion, optimizer, x_train, y_train, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c6587f93-5147-4b7e-b62f-68145e8a8489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Приближенное значение интеграла: 0.420580\n"
     ]
    }
   ],
   "source": [
    "# Вычисление интеграла\n",
    "integral = integrate_model(model)\n",
    "print(f'Приближенное значение интеграла: {integral:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "812e175e-6892-4699-88e7-c8d947388a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аналитическое значение интеграла нейросети: 0.420595\n"
     ]
    }
   ],
   "source": [
    "# Вычисление интеграла аналитически\n",
    "integral_analytical = integrate_model_analytically(model)\n",
    "print(f'Аналитическое значение интеграла нейросети: {integral_analytical:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa04572f-3a78-446f-a8dc-49a6c7ffb363",
   "metadata": {},
   "source": [
    "## 2D $sin(x) + cos(y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7551e61a-26c3-4e46-a1dc-5bcd1116fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_model_2d_analytically(model, x_min, x_max, y_min, y_max):\n",
    "    \"\"\"\n",
    "    Интегрирует нейронную сеть с 2 входами по обеим переменным аналитически.\n",
    "    \"\"\"\n",
    "    # Извлечение параметров сети\n",
    "    with torch.no_grad():\n",
    "        W1 = model.input_layer.weight  # W^{(1)}: веса входного слоя\n",
    "        b1 = model.input_layer.bias    # b^{(1)}: смещения входного слоя\n",
    "        W2 = model.output_layer.weight  # W^{(2)}: веса выходного слоя\n",
    "        b2 = model.output_layer.bias    # b^{(2)}: смещение выходного слоя\n",
    "\n",
    "    # Сигмоида и её интеграл\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "    def sigmoid_integral(a, b, x_min, x_max):\n",
    "        \"\"\"\n",
    "        Вычисляет аналитический интеграл сигмоиды: ∫ σ(ax + b) dx\n",
    "        \"\"\"\n",
    "        if a == 0:\n",
    "            raise ValueError(\"Параметр 'a' не может быть равен нулю для корректного вычисления.\")\n",
    "        \n",
    "        # Преобразуем все параметры в тензоры, чтобы обеспечить правильную работу с PyTorch\n",
    "        a = torch.tensor(a, dtype=torch.float32)\n",
    "        b = torch.tensor(b, dtype=torch.float32)\n",
    "        x_min = torch.tensor(x_min, dtype=torch.float32)\n",
    "        x_max = torch.tensor(x_max, dtype=torch.float32)\n",
    "    \n",
    "        return (1 / a) * (\n",
    "            torch.log(1 + torch.exp(a * x_max + b)) - torch.log(1 + torch.exp(a * x_min + b))\n",
    "        )\n",
    "\n",
    "\n",
    "    # Интеграл: ∫ f(x, y) dx dy\n",
    "    integral = b2.item() * (x_max - x_min) * (y_max - y_min)  # Член b^{(2)}\n",
    "\n",
    "    for i in range(W2.size(1)):  # По скрытым нейронам\n",
    "        weight = W2[0, i].item()  # W^{(2)}\n",
    "\n",
    "        # Интегрируем по каждой переменной\n",
    "        for j_x, j_y in [(0, 0), (0, 1), (1, 0), (1, 1)]:  # Индексы весов W^{(1)}\n",
    "            a_x = W1[i, j_x].item()  # Вес для x\n",
    "            a_y = W1[i, j_y].item()  # Вес для y\n",
    "            b = b1[i].item()         # Смещение\n",
    "\n",
    "            # Интеграция по y\n",
    "            inner_integral = sigmoid_integral(a_y, b, y_min, y_max)\n",
    "            # Интеграция по x\n",
    "            integral += weight * sigmoid_integral(a_x, inner_integral, x_min, x_max)\n",
    "\n",
    "    return integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6e0e059f-03f7-4dcd-9a2c-c4519405c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гиперпараметры\n",
    "input_dim = 2  # Два входа: x и y\n",
    "hidden_dim = 10  # Размер скрытого слоя\n",
    "output_dim = 1  # Один выход\n",
    "learning_rate = 0.01\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4c664a8d-a980-4f64-b327-e26f621edc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель\n",
    "model = MLP(input_dim, hidden_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0b0a6ba7-5b91-493b-ade6-77df222f661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2D_data(num_samples):\n",
    "    x = torch.rand(num_samples, 1)\n",
    "    y = torch.rand(num_samples, 1)\n",
    "    f_values = torch.sin(x) + torch.cos(y)\n",
    "    return torch.cat((x, y), dim=1), f_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b91a23cb-f6d0-4139-83ae-def8b5c8b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация данных\n",
    "X_train, y_train = generate_2D_data(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "daf055fb-6ee3-445e-9cda-1dddc805f5d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/10000], Loss: 0.0813\n",
      "Epoch [100/10000], Loss: 0.0581\n",
      "Epoch [150/10000], Loss: 0.0316\n",
      "Epoch [200/10000], Loss: 0.0182\n",
      "Epoch [250/10000], Loss: 0.0065\n",
      "Epoch [300/10000], Loss: 0.0025\n",
      "Epoch [350/10000], Loss: 0.0014\n",
      "Epoch [400/10000], Loss: 0.0011\n",
      "Epoch [450/10000], Loss: 0.0012\n",
      "Epoch [500/10000], Loss: 0.0011\n",
      "Epoch [550/10000], Loss: 0.0012\n",
      "Epoch [600/10000], Loss: 0.0012\n",
      "Epoch [650/10000], Loss: 0.0011\n",
      "Epoch [700/10000], Loss: 0.0011\n",
      "Epoch [750/10000], Loss: 0.0011\n",
      "Epoch [800/10000], Loss: 0.0011\n",
      "Epoch [850/10000], Loss: 0.0011\n",
      "Epoch [900/10000], Loss: 0.0010\n",
      "Epoch [950/10000], Loss: 0.0011\n",
      "Epoch [1000/10000], Loss: 0.0011\n",
      "Epoch [1050/10000], Loss: 0.0011\n",
      "Epoch [1100/10000], Loss: 0.0011\n",
      "Epoch [1150/10000], Loss: 0.0011\n",
      "Epoch [1200/10000], Loss: 0.0010\n",
      "Epoch [1250/10000], Loss: 0.0010\n",
      "Epoch [1300/10000], Loss: 0.0010\n",
      "Epoch [1350/10000], Loss: 0.0010\n",
      "Epoch [1400/10000], Loss: 0.0011\n",
      "Epoch [1450/10000], Loss: 0.0010\n",
      "Epoch [1500/10000], Loss: 0.0010\n",
      "Epoch [1550/10000], Loss: 0.0011\n",
      "Epoch [1600/10000], Loss: 0.0010\n",
      "Epoch [1650/10000], Loss: 0.0010\n",
      "Epoch [1700/10000], Loss: 0.0010\n",
      "Epoch [1750/10000], Loss: 0.0010\n",
      "Epoch [1800/10000], Loss: 0.0010\n",
      "Epoch [1850/10000], Loss: 0.0010\n",
      "Epoch [1900/10000], Loss: 0.0010\n",
      "Epoch [1950/10000], Loss: 0.0009\n",
      "Epoch [2000/10000], Loss: 0.0009\n",
      "Epoch [2050/10000], Loss: 0.0010\n",
      "Epoch [2100/10000], Loss: 0.0009\n",
      "Epoch [2150/10000], Loss: 0.0009\n",
      "Epoch [2200/10000], Loss: 0.0009\n",
      "Epoch [2250/10000], Loss: 0.0010\n",
      "Epoch [2300/10000], Loss: 0.0009\n",
      "Epoch [2350/10000], Loss: 0.0009\n",
      "Epoch [2400/10000], Loss: 0.0008\n",
      "Epoch [2450/10000], Loss: 0.0008\n",
      "Epoch [2500/10000], Loss: 0.0009\n",
      "Epoch [2550/10000], Loss: 0.0008\n",
      "Epoch [2600/10000], Loss: 0.0007\n",
      "Epoch [2650/10000], Loss: 0.0007\n",
      "Epoch [2700/10000], Loss: 0.0007\n",
      "Epoch [2750/10000], Loss: 0.0006\n",
      "Epoch [2800/10000], Loss: 0.0006\n",
      "Epoch [2850/10000], Loss: 0.0005\n",
      "Epoch [2900/10000], Loss: 0.0005\n",
      "Epoch [2950/10000], Loss: 0.0004\n",
      "Epoch [3000/10000], Loss: 0.0003\n",
      "Epoch [3050/10000], Loss: 0.0003\n",
      "Epoch [3100/10000], Loss: 0.0003\n",
      "Epoch [3150/10000], Loss: 0.0002\n",
      "Epoch [3200/10000], Loss: 0.0002\n",
      "Epoch [3250/10000], Loss: 0.0001\n",
      "Epoch [3300/10000], Loss: 0.0001\n",
      "Epoch [3350/10000], Loss: 0.0001\n",
      "Epoch [3400/10000], Loss: 0.0001\n",
      "Epoch [3450/10000], Loss: 0.0001\n",
      "Epoch [3500/10000], Loss: 0.0001\n",
      "Epoch [3550/10000], Loss: 0.0001\n",
      "Epoch [3600/10000], Loss: 0.0001\n",
      "Epoch [3650/10000], Loss: 0.0000\n",
      "Epoch [3700/10000], Loss: 0.0000\n",
      "Epoch [3750/10000], Loss: 0.0000\n",
      "Epoch [3800/10000], Loss: 0.0000\n",
      "Epoch [3850/10000], Loss: 0.0000\n",
      "Epoch [3900/10000], Loss: 0.0000\n",
      "Epoch [3950/10000], Loss: 0.0000\n",
      "Epoch [4000/10000], Loss: 0.0000\n",
      "Epoch [4050/10000], Loss: 0.0000\n",
      "Epoch [4100/10000], Loss: 0.0000\n",
      "Epoch [4150/10000], Loss: 0.0000\n",
      "Epoch [4200/10000], Loss: 0.0000\n",
      "Epoch [4250/10000], Loss: 0.0000\n",
      "Epoch [4300/10000], Loss: 0.0000\n",
      "Epoch [4350/10000], Loss: 0.0000\n",
      "Epoch [4400/10000], Loss: 0.0000\n",
      "Epoch [4450/10000], Loss: 0.0000\n",
      "Epoch [4500/10000], Loss: 0.0000\n",
      "Epoch [4550/10000], Loss: 0.0000\n",
      "Epoch [4600/10000], Loss: 0.0000\n",
      "Epoch [4650/10000], Loss: 0.0000\n",
      "Epoch [4700/10000], Loss: 0.0000\n",
      "Epoch [4750/10000], Loss: 0.0000\n",
      "Epoch [4800/10000], Loss: 0.0000\n",
      "Epoch [4850/10000], Loss: 0.0000\n",
      "Epoch [4900/10000], Loss: 0.0000\n",
      "Epoch [4950/10000], Loss: 0.0000\n",
      "Epoch [5000/10000], Loss: 0.0000\n",
      "Epoch [5050/10000], Loss: 0.0000\n",
      "Epoch [5100/10000], Loss: 0.0000\n",
      "Epoch [5150/10000], Loss: 0.0000\n",
      "Epoch [5200/10000], Loss: 0.0000\n",
      "Epoch [5250/10000], Loss: 0.0000\n",
      "Epoch [5300/10000], Loss: 0.0000\n",
      "Epoch [5350/10000], Loss: 0.0000\n",
      "Epoch [5400/10000], Loss: 0.0000\n",
      "Epoch [5450/10000], Loss: 0.0000\n",
      "Epoch [5500/10000], Loss: 0.0000\n",
      "Epoch [5550/10000], Loss: 0.0000\n",
      "Epoch [5600/10000], Loss: 0.0000\n",
      "Epoch [5650/10000], Loss: 0.0000\n",
      "Epoch [5700/10000], Loss: 0.0000\n",
      "Epoch [5750/10000], Loss: 0.0000\n",
      "Epoch [5800/10000], Loss: 0.0000\n",
      "Epoch [5850/10000], Loss: 0.0000\n",
      "Epoch [5900/10000], Loss: 0.0000\n",
      "Epoch [5950/10000], Loss: 0.0000\n",
      "Epoch [6000/10000], Loss: 0.0000\n",
      "Epoch [6050/10000], Loss: 0.0000\n",
      "Epoch [6100/10000], Loss: 0.0000\n",
      "Epoch [6150/10000], Loss: 0.0000\n",
      "Epoch [6200/10000], Loss: 0.0000\n",
      "Epoch [6250/10000], Loss: 0.0000\n",
      "Epoch [6300/10000], Loss: 0.0000\n",
      "Epoch [6350/10000], Loss: 0.0000\n",
      "Epoch [6400/10000], Loss: 0.0000\n",
      "Epoch [6450/10000], Loss: 0.0000\n",
      "Epoch [6500/10000], Loss: 0.0000\n",
      "Epoch [6550/10000], Loss: 0.0000\n",
      "Epoch [6600/10000], Loss: 0.0000\n",
      "Epoch [6650/10000], Loss: 0.0000\n",
      "Epoch [6700/10000], Loss: 0.0000\n",
      "Epoch [6750/10000], Loss: 0.0000\n",
      "Epoch [6800/10000], Loss: 0.0000\n",
      "Epoch [6850/10000], Loss: 0.0000\n",
      "Epoch [6900/10000], Loss: 0.0000\n",
      "Epoch [6950/10000], Loss: 0.0000\n",
      "Epoch [7000/10000], Loss: 0.0000\n",
      "Epoch [7050/10000], Loss: 0.0000\n",
      "Epoch [7100/10000], Loss: 0.0000\n",
      "Epoch [7150/10000], Loss: 0.0000\n",
      "Epoch [7200/10000], Loss: 0.0000\n",
      "Epoch [7250/10000], Loss: 0.0000\n",
      "Epoch [7300/10000], Loss: 0.0000\n",
      "Epoch [7350/10000], Loss: 0.0000\n",
      "Epoch [7400/10000], Loss: 0.0000\n",
      "Epoch [7450/10000], Loss: 0.0000\n",
      "Epoch [7500/10000], Loss: 0.0000\n",
      "Epoch [7550/10000], Loss: 0.0000\n",
      "Epoch [7600/10000], Loss: 0.0000\n",
      "Epoch [7650/10000], Loss: 0.0000\n",
      "Epoch [7700/10000], Loss: 0.0000\n",
      "Epoch [7750/10000], Loss: 0.0000\n",
      "Epoch [7800/10000], Loss: 0.0000\n",
      "Epoch [7850/10000], Loss: 0.0000\n",
      "Epoch [7900/10000], Loss: 0.0000\n",
      "Epoch [7950/10000], Loss: 0.0000\n",
      "Epoch [8000/10000], Loss: 0.0000\n",
      "Epoch [8050/10000], Loss: 0.0000\n",
      "Epoch [8100/10000], Loss: 0.0000\n",
      "Epoch [8150/10000], Loss: 0.0000\n",
      "Epoch [8200/10000], Loss: 0.0000\n",
      "Epoch [8250/10000], Loss: 0.0000\n",
      "Epoch [8300/10000], Loss: 0.0000\n",
      "Epoch [8350/10000], Loss: 0.0000\n",
      "Epoch [8400/10000], Loss: 0.0000\n",
      "Epoch [8450/10000], Loss: 0.0000\n",
      "Epoch [8500/10000], Loss: 0.0000\n",
      "Epoch [8550/10000], Loss: 0.0000\n",
      "Epoch [8600/10000], Loss: 0.0000\n",
      "Epoch [8650/10000], Loss: 0.0000\n",
      "Epoch [8700/10000], Loss: 0.0000\n",
      "Epoch [8750/10000], Loss: 0.0000\n",
      "Epoch [8800/10000], Loss: 0.0000\n",
      "Epoch [8850/10000], Loss: 0.0000\n",
      "Epoch [8900/10000], Loss: 0.0000\n",
      "Epoch [8950/10000], Loss: 0.0000\n",
      "Epoch [9000/10000], Loss: 0.0000\n",
      "Epoch [9050/10000], Loss: 0.0000\n",
      "Epoch [9100/10000], Loss: 0.0000\n",
      "Epoch [9150/10000], Loss: 0.0000\n",
      "Epoch [9200/10000], Loss: 0.0000\n",
      "Epoch [9250/10000], Loss: 0.0000\n",
      "Epoch [9300/10000], Loss: 0.0000\n",
      "Epoch [9350/10000], Loss: 0.0000\n",
      "Epoch [9400/10000], Loss: 0.0000\n",
      "Epoch [9450/10000], Loss: 0.0000\n",
      "Epoch [9500/10000], Loss: 0.0000\n",
      "Epoch [9550/10000], Loss: 0.0000\n",
      "Epoch [9600/10000], Loss: 0.0000\n",
      "Epoch [9650/10000], Loss: 0.0000\n",
      "Epoch [9700/10000], Loss: 0.0000\n",
      "Epoch [9750/10000], Loss: 0.0000\n",
      "Epoch [9800/10000], Loss: 0.0000\n",
      "Epoch [9850/10000], Loss: 0.0000\n",
      "Epoch [9900/10000], Loss: 0.0000\n",
      "Epoch [9950/10000], Loss: 0.0000\n",
      "Epoch [10000/10000], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Генерация данных\n",
    "    X_train, y_train = generate_data(1000)\n",
    "    \n",
    "    # Обучение\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "856c5c68-adf8-4509-a5d9-fa24b0173014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аналитическое значение двойного интеграла нейросети: 3.633325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/df/v02b0vfx2tbc5plrdx4v6gk80000gn/T/ipykernel_14564/2003117665.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b = torch.tensor(b, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Пределы интегрирования\n",
    "x_min, x_max = 0, 1\n",
    "y_min, y_max = 0, 1\n",
    "\n",
    "# Вычисление интеграла нейросети\n",
    "integral_2d_analytical = integrate_model_2d_analytically(model, x_min, x_max, y_min, y_max)\n",
    "print(f'Аналитическое значение двойного интеграла нейросети: {integral_2d_analytical:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "76642646-bbd2-4c67-94f5-301d99bd1605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение аналитического интеграла: 1.301169\n"
     ]
    }
   ],
   "source": [
    "# Значения для cos(1) и sin(1)\n",
    "cos_1 = np.cos(1)\n",
    "sin_1 = np.sin(1)\n",
    "\n",
    "# Аналитический интеграл\n",
    "true_integral = (1 - cos_1) + sin_1\n",
    "print(f\"Значение аналитического интеграла: {true_integral:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dc43db4a-d54e-4831-a21b-b8e443ceaaab",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x1 and 2x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Вычисление интеграла\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m integral \u001b[38;5;241m=\u001b[39m \u001b[43mintegrate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mПриближенное значение интеграла: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mintegral\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m, in \u001b[0;36mintegrate_model\u001b[0;34m(model, n_samples)\u001b[0m\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, n_samples)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Точки для интеграции\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m integral \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtrapz(y\u001b[38;5;241m.\u001b[39msqueeze(), x\u001b[38;5;241m.\u001b[39msqueeze())  \u001b[38;5;66;03m# Метод трапеций\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m integral\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Programming/JUPITER/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/JUPITER/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer(x)\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n",
      "File \u001b[0;32m~/Programming/JUPITER/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/JUPITER/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Programming/JUPITER/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x1 and 2x10)"
     ]
    }
   ],
   "source": [
    "# Вычисление интеграла\n",
    "integral = integrate_model(model)\n",
    "print(f'Приближенное значение интеграла: {integral:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51253cb-7621-41ec-b010-3ee265dc7e59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
