{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b8e4a6-fccf-4f94-bcbd-dba83e6fb6fa",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Change the moment when training and test data is generated: they should be generated silmutaniousely\n",
    "2. Improve approximation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4033090-f3f4-4891-af06-d00a907355b6",
   "metadata": {},
   "source": [
    "# Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68836988-588a-4929-bfea-e24c717c1099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpmath import polylog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66214e6f-fe49-4538-931e-8b29fda25a61",
   "metadata": {},
   "source": [
    "# Dataset related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "080e0c47-d99c-4619-b8e1-ca64997e0db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(func, n_samples=100, n_dim=1):\n",
    "    \"\"\"\n",
    "        Generates data in the form of a 2D (1D in case if the function \n",
    "        is single-variable) tensor of variables for fuction and neural network input\n",
    "        as well as te function values for the generated tensor of variables.\n",
    "\n",
    "        @param func      function to provide values for the variables\n",
    "        @param n_samples number of points of data to generate\n",
    "                         (default value is 100)\n",
    "        @param n_dim     number of dimension of the function func\n",
    "                         (default value is 1)\n",
    "    \"\"\"\n",
    "    X = torch.linspace(0, 1, n_samples * n_dim).view(n_samples, n_dim)\n",
    "    y = func(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c043ec8b-ea03-4278-ba7a-0bbe30a5d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(X):\n",
    "    return torch.cos(X[:, 0]) + torch.sin(X[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "403992b4-93cd-4758-bcd7-6eed0dd608d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indefinite_integral_f1(X):\n",
    "    # Extract x1 and x2 from the columns of the 2D tensor X\n",
    "    x1 = X[:, 0]  # First column\n",
    "    x2 = X[:, 1]  # Second column\n",
    "\n",
    "    return -x1 * torch.cos(x2) + x2 * torch.sin(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df20f17e-ea02-422c-b30c-f7e40bf64d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def definite_integral_f1(A, B):\n",
    "\n",
    "    return indefinite_integral_f1(B) - indefinite_integral_f1(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d95658-8831-40b9-bf21-7f8221bbc3cc",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d31bc935-0da1-47c8-88c3-e7b632df3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"\n",
    "            Neural network constructor for a multi-layer perceptron.\n",
    "            \n",
    "            @param input_size  Number of features in the input data\n",
    "            @param hidden_size Number of neurons in the hidden layer\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_hidden_layer = nn.Linear(input_size, hidden_size)  # input -> hidden\n",
    "        self.sigmoid_activation = nn.Sigmoid()  # Activation function\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)  # hidden -> output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass through the network.\n",
    "            \n",
    "            @param x  Input data\n",
    "            @returns   The network's output\n",
    "        \"\"\"\n",
    "        x = self.input_hidden_layer(x)  # Pass through input-hidden layer\n",
    "        x = self.sigmoid_activation(x)  # Apply activation function\n",
    "        x = self.output_layer(x)  # Pass through output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a17f2896-175c-4069-9f26-d8102be34593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, x_train, y_train, epochs):\n",
    "    \"\"\"\n",
    "        Trains the model.\n",
    "        \n",
    "        @param model        The model to be trained\n",
    "        @param criterion    Loss function\n",
    "        @param optimizer    Optimization algorithm\n",
    "        @param x_train      Training inputs\n",
    "        @param y_train      True labels\n",
    "        @param epochs       Number of training epochs\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for epoch in range(epochs):\n",
    "        predictions = model(x_train)  # Forward pass\n",
    "        loss = criterion(predictions, y_train)  # Compute the loss\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.10f}')\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39dba2eb-ddbc-40cf-b227-d1e150299b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion, x_test, y_test):\n",
    "    \"\"\"\n",
    "        Tests the model.\n",
    "        \n",
    "        @param model        The trained model\n",
    "        @param criterion    Loss function\n",
    "        @param x_test       Test inputs\n",
    "        @param y_test       True labels\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # Disable gradient calculation for testing\n",
    "        predictions = model(x_test)  # Forward pass\n",
    "        loss = criterion(predictions, y_test)  # Compute the loss\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30eab7f1-53b1-4b97-b9f4-5c06b3e1a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_model(model, x_test):\n",
    "    \"\"\"\n",
    "        Uses the model to predict values based on x_test arguments.\n",
    "    \n",
    "        @param model        The trained model\n",
    "        @param x_test       Test inputs\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        predictions = model(x_test)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f621f94-09f0-4dfe-84e2-b13c71d5baad",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8afab6a-15b0-4802-b363-8896b7959e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 20\n",
    "n_samples = 10000  # Number of samples for training\n",
    "epochs = 1000  # Number of epochs\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2efdd7a4-f6be-4288-824a-85df561ee101",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_data(f1, n_samples=n_samples, n_dim=input_size) # Training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345abb9c-8657-4dbf-a97a-540ba068241c",
   "metadata": {},
   "source": [
    "# Model definition and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00079878-b655-43a6-aecc-5d5b054dfe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 20]              60\n",
      "           Sigmoid-2                   [-1, 20]               0\n",
      "            Linear-3                    [-1, 1]              21\n",
      "================================================================\n",
      "Total params: 81\n",
      "Trainable params: 81\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_size, hidden_size)\n",
    "\n",
    "summary(model, input_size=(input_size, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab6a60-a303-44ae-8bef-6c82e73716c0",
   "metadata": {},
   "source": [
    "# Model compilation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a0855-2e3c-43e0-97d0-c025adef8ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([10000])) that is different to the input size (torch.Size([10000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.2305060625\n",
      "Epoch [200/1000], Loss: 0.0263075177\n",
      "Epoch [300/1000], Loss: 0.0222569294\n",
      "Epoch [400/1000], Loss: 0.0213388819\n",
      "Epoch [500/1000], Loss: 0.0204103440\n",
      "Epoch [600/1000], Loss: 0.0195100084\n",
      "Epoch [700/1000], Loss: 0.0186702684\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_history = train_model(model, criterion, optimizer, X_train, y_train, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4036b0-9045-4f04-ac4d-9969a21f44fa",
   "metadata": {},
   "source": [
    "# Learning curve (based on MSE function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5062b4-87b6-4472-a74c-6306b1d37cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_history, label='Loss', color='b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Function over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf105ca-5a2b-4cc5-84e7-00461d6cd706",
   "metadata": {},
   "source": [
    "# Model's testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74993e10-86bf-42f5-8c4b-c3091f346177",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = generate_data(f1, 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613d9b6-d39a-4acd-abe1-ee49d67b6588",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = test_model(model, criterion, x_test, y_test)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ffe7f-2ab6-4c00-bfe6-d55707d32f80",
   "metadata": {},
   "source": [
    "# Function to extract model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b22fb7-7cc1-4dff-a138-58622f2c5566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_params(model):\n",
    "    \"\"\"\n",
    "        Extract's model's parameters (weights and biases for each layer)\n",
    "        \n",
    "        @param    model the model from which parameters will be extracted\n",
    "        \n",
    "        @returns  tuple of 4 numpy.ndarray: bias-1, weights-1 (for all the inputs; \n",
    "                  it can be a 1D or 2D tensor depending on the number of NN inputs), \n",
    "                  bias-2 and weights-2 (numer at the end represents layer).\n",
    "    \"\"\"\n",
    "    # detach() - takes the required params and turns them into a numpy.ndarray\n",
    "    # flatten() - flattens multidimensional tensors into 1D tensor\n",
    "    b1 = model.input_hidden_layer.bias.detach().numpy() \n",
    "    w1 = model.input_hidden_layer.weight.detach().numpy()\n",
    "    b2 = model.output_layer.bias.detach().numpy() \n",
    "    w2 = model.output_layer.weight.detach().numpy().flatten()\n",
    "    \n",
    "    return b1, w1, b2, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214a5e4-8da0-4dc1-990c-7504a2cc5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1, w1, b2, w2 = extract_model_params(model)\n",
    "b1.shape, w1.shape, b2.shape, w2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be0685-e795-4e5c-a46d-3b49d0984270",
   "metadata": {},
   "source": [
    "# Function of numerical integration based on model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206b6ae1-17e3-4c33-9376-874f9f6e8f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NN_integral(alpha1, alpha2, beta1, beta2, b1, w1, b2, w2):\n",
    "    \"\"\"\n",
    "    Function that implements the new analytical integral based on the provided formulae.\n",
    "    \n",
    "    @param alpha1  Lower limit of integration for the first dimension\n",
    "    @param alpha2  Lower limit of integration for the second dimension\n",
    "    @param beta1   Upper limit of integration for the first dimension\n",
    "    @param beta2   Upper limit of integration for the second dimension\n",
    "    @param b1      Shifts between input and hidden layers\n",
    "    @param w1      Weights between input and hidden layers\n",
    "    @param b2      Shifts between hidden and output layers\n",
    "    @param w2      Weights between hidden and output layers\n",
    "\n",
    "    @returns       The analytical integral based on the neural network parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def Phi_j(alpha1, alpha2, beta1, beta2, b1_j, w1_j):\n",
    "        \"\"\"\n",
    "        Nested function implementing the difference of polylogarithms as defined in the new formula.\n",
    "        \n",
    "        @param alpha1  Lower limit of integration for the first dimension\n",
    "        @param alpha2  Lower limit of integration for the second dimension\n",
    "        @param beta1   Upper limit of integration for the first dimension\n",
    "        @param beta2   Upper limit of integration for the second dimension\n",
    "        @param b1_j    j-th shift between input and hidden layers\n",
    "        @param w1_j    j-th weight between input and hidden layers\n",
    "        @param w2_j    j-th weight between hidden and output layers\n",
    "        \n",
    "        @returns       Difference of polylogarithms as defined in the new formula.\n",
    "        \"\"\"\n",
    "        w1_1j = w1_j[0]\n",
    "        w1_2j = w1_j[1]\n",
    "\n",
    "        term_alpha1_alpha1 = polylog(2, -np.exp(-b1_j - w1_1j * alpha1 - w1_2j * alpha1))  # Li_2 term for alpha1, alpha1\n",
    "        term_alpha1_beta2  = polylog(2, -np.exp(-b1_j - w1_1j * alpha1 - w1_2j * beta2))   # Li_2 term for alpha1, beta2\n",
    "        term_beta1_alpha2  = polylog(2, -np.exp(-b1_j - w1_1j * beta1 - w1_2j * alpha2))    # Li_2 term for beta1, alpha2\n",
    "        term_beta1_beta2   = polylog(2, -np.exp(-b1_j - w1_1j * beta1 - w1_2j * beta2))     # Li_2 term for beta1, beta2\n",
    "        \n",
    "        return term_alpha1_alpha1 - term_alpha1_beta2 - term_beta1_alpha2 - term_beta1_beta2\n",
    "\n",
    "    integral_sum = 0  # Sum of integrals\n",
    "    \n",
    "    # Sum over all j\n",
    "    for w2_j, w1_j, b1_j in zip(w2, w1, b1):\n",
    "        phi_j = Phi_j(alpha1, alpha2, beta1, beta2, b1_j, w1_j)  # Compute the difference of polylogarithms\n",
    "        integral_sum += w2_j * ((beta1 - alpha1) * (beta2 - alpha2) + (phi_j / (w1_j[0] * w1_j[1])))  # Accumulate integral for sum\n",
    "    \n",
    "    # Final result based on the new formula\n",
    "    return b2 * (beta1 - alpha1) * (beta2 - alpha2) + integral_sum  # Full formula result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066d46f-e685-42cb-a391-0de80db86bb5",
   "metadata": {},
   "source": [
    "# NNI evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbaef81-094d-48ca-b320-47b1d2ecce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "intgr = get_NN_integral(0.2, 0.2, 0.5, 0.5, b1, w1, b2, w2)\n",
    "intgr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
